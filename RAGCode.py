from fastapi import FastAPI, UploadFile, File, Form
from fastapi.responses import JSONResponse
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate
from langchain_openai import ChatOpenAI
import os
from docx import Document
import tempfile
from dotenv import load_dotenv

# Environment variables are loaded from .env file if it exists 
load_dotenv()

# OpenAI API key is retrieved from environment variables
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

if OPENAI_API_KEY is None:
    raise ValueError("OPENAI_API_KEY not set in environment variables.")

app = FastAPI()

# Prompt template is defined for summarization
summarization_prompt_template = PromptTemplate(
    input_variables=["text"],
    template="You are an assistant for summarizing conversations. Summarize the following conversation in three sentences, highlighting the main points and any conclusions reached.\nConversation:\n{text}"
)

# Prompt template is defined for answering a question
question_prompt_template = PromptTemplate(
    input_variables=["text", "question"],
    template="Answer the following question based on the conversation:\nConversation: {text}\nQuestion: {question}"
)

# ChatOpenAI LLM instance is created
llm = ChatOpenAI(openai_api_key=OPENAI_API_KEY, model_name="gpt-4o", temperature=0)

# Summarization and question-answering chains are created
summarization_chain = LLMChain(prompt=summarization_prompt_template, llm=llm)
question_chain = LLMChain(prompt=question_prompt_template, llm=llm)

def read_docx(file_path):
    """Read text from a .docx file."""
    doc = Document(file_path)
    full_text = []
    for para in doc.paragraphs:
        full_text.append(para.text)
    return '\n'.join(full_text)

@app.post("/summarize")
async def summarize(file: UploadFile = File(...)):
    """Summarize the contents of a .docx file."""
    # Create a temporary file
    with tempfile.NamedTemporaryFile(delete=False) as tmp:
        file_location = tmp.name
        tmp.write(await file.read())

    # Text from the .docx file is read
    text = read_docx(file_location)

    # Summary is generated by the use of LLM chain
    summary = summarization_chain.run({"text": text})

    # Temporary file is cleaned up
    os.remove(file_location)

    return JSONResponse(content={"summary": summary})

@app.post("/ask-question")
async def ask_question(file: UploadFile = File(...), question: str = Form(...)):
    """Answer a question based on the contents of a .docx file."""
    # Temporary file is created
    with tempfile.NamedTemporaryFile(delete=False) as tmp:
        file_location = tmp.name
        tmp.write(await file.read())

    # Text from the .docx file is read
    text = read_docx(file_location)

    # Answer to the question is generated by the use of LLM chain
    answer = question_chain.run({"text": text, "question": question})

    # Temporary file is cleaned up
    os.remove(file_location)

    return JSONResponse(content={"answer": answer})

if __name__ == "__main__":
    import uvicorn
    import nest_asyncio
    nest_asyncio.apply()
    uvicorn.run(app, host="0.0.0.0", port=8000)
